{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT SQuAD v2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/framoni/jup-notebooks/blob/master/BERT_SQuAD_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJNKA5PWzhqs",
        "colab_type": "text"
      },
      "source": [
        "# Fine-tuning BERT on SQuAD v2 dataset for question answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeRNFLFr4ehq",
        "colab_type": "text"
      },
      "source": [
        "This notebook reproduces from scratch and without concerns the fine-tuning of BERT on the SQuAD v2 dataset for question answering tasks using the models uploaded on TensorFlow Hub. Just run the notebook and don't worry about details!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj2worJVzTW4",
        "colab_type": "text"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mshLHDcwSe0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install bert-tensorflow\n",
        "\n",
        "import bert\n",
        "from bert import modeling\n",
        "from bert import optimization\n",
        "from bert import run_squad\n",
        "from bert import tokenization\n",
        "import collections\n",
        "import datetime\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import six\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PsTb6lGF1h0",
        "colab_type": "text"
      },
      "source": [
        "## Set root folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRnnvhA5LjOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if use_drive = False, upload SQuAD \"train-v2.0.json\" \"dev-v2.0.json\" on Colab\n",
        "# if use_drive = True, upload those files in a folder named \"BERT SQuAD\" in your Google Drive\n",
        "\n",
        "use_drive = False\n",
        "\n",
        "if use_drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  root  = '/content/drive/My Drive/BERT SQuAD v2'\n",
        "else:\n",
        "  root = ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA2ktuPTmbar",
        "colab_type": "text"
      },
      "source": [
        "## Settings and model parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HicikOcdmYp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BERT-large\n",
        "\n",
        "# bert_model_hub = 'https://tfhub.dev/google/bert_uncased_L-24_H-1024_A-16/1'\n",
        "\n",
        "# BERT-small\n",
        "\n",
        "bert_model_hub = 'https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1'\n",
        "\n",
        "output_dir = 'output'\n",
        "do_delete = True # whether do delete the content of output folder if it already exists\n",
        "input_file = os.path.join(root, 'train-v2.0.json')\n",
        "\n",
        "batch_size = 32\n",
        "predict_batch_size = 8\n",
        "learning_rate = 5e-5\n",
        "num_train_epochs = 3.0\n",
        "warmup_proportion = 0.1\n",
        "\n",
        "max_seq_length = 384 # if OOM try 128\n",
        "max_query_length = 64\n",
        "max_answer_length = 30\n",
        "\n",
        "doc_stride = 128 # stride of the window sliding through the length of the document\n",
        "null_score_diff_threshold = 0.0 # threshold on the difference (null_score - best_non_null) over which to predict null\n",
        "n_best_size = 3 # number of n-best predictions to generate in the nbest_predictions.json output file\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO) # set TF verbosity\n",
        "\n",
        "save_checkpoints_steps = 1000\n",
        "save_summary_steps = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKrdESZgmmw4",
        "colab_type": "text"
      },
      "source": [
        "## Create output folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXahxo266Ra9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if do_delete:\n",
        "  try:\n",
        "    tf.gfile.DeleteRecursively(output_dir)\n",
        "  except:\n",
        "    pass\n",
        "tf.gfile.MakeDirs(output_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5ia0wkqncWm",
        "colab_type": "text"
      },
      "source": [
        "## Create BERT tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW7Nk0f0nyVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"\n",
        "  Get the vocab file and casing info from the Hub module\n",
        "  \"\"\"\n",
        "  \n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(bert_model_hub)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"], tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return [do_lower_case, bert.tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)]\n",
        "\n",
        "[do_lower_case, tokenizer] = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W8BrfcAm7Ot",
        "colab_type": "text"
      },
      "source": [
        "## Load training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2PqtNcZSnZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_examples(input_file, is_training):\n",
        "  \"\"\"\n",
        "  This function is an extension of run_squad.read_squad_examples to allow for input of external text files as paragraphs\n",
        "  \"\"\"\n",
        "\n",
        "  with tf.gfile.Open(input_file, \"r\") as reader:\n",
        "    input_data = json.load(reader)[\"data\"]\n",
        "\n",
        "  def is_whitespace(c):\n",
        "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  examples = []\n",
        "  for entry in input_data:\n",
        "    for paragraph in entry[\"paragraphs\"]:\n",
        "      paragraph_text = paragraph[\"context\"]\n",
        "      if os.path.isfile(paragraph_text):\n",
        "            with open(paragraph_text, \"r\", encoding=\"utf-8\") as f:\n",
        "                paragraph_text = f.read()\n",
        "      doc_tokens = []\n",
        "      char_to_word_offset = []\n",
        "      prev_is_whitespace = True\n",
        "      for c in paragraph_text:\n",
        "        if is_whitespace(c):\n",
        "          prev_is_whitespace = True\n",
        "        else:\n",
        "          if prev_is_whitespace:\n",
        "            doc_tokens.append(c)\n",
        "          else:\n",
        "            doc_tokens[-1] += c\n",
        "          prev_is_whitespace = False\n",
        "        char_to_word_offset.append(len(doc_tokens) - 1)\n",
        "      for qa in paragraph[\"qas\"]:\n",
        "        qas_id = qa[\"id\"]\n",
        "        question_text = qa[\"question\"]\n",
        "        start_position = None\n",
        "        end_position = None\n",
        "        orig_answer_text = None\n",
        "        is_impossible = False\n",
        "        if is_training:\n",
        "          is_impossible = qa[\"is_impossible\"]\n",
        "          if (len(qa[\"answers\"]) != 1) and (not is_impossible):\n",
        "            raise ValueError(\n",
        "                \"For training, each question should have exactly 1 answer.\")\n",
        "          if not is_impossible:\n",
        "            answer = qa[\"answers\"][0]\n",
        "            orig_answer_text = answer[\"text\"]\n",
        "            answer_offset = answer[\"answer_start\"]\n",
        "            answer_length = len(orig_answer_text)\n",
        "            start_position = char_to_word_offset[answer_offset]\n",
        "            end_position = char_to_word_offset[answer_offset + answer_length - 1]\n",
        "            actual_text = \" \".join(\n",
        "                doc_tokens[start_position:(end_position + 1)])\n",
        "            cleaned_answer_text = \" \".join(\n",
        "                tokenization.whitespace_tokenize(orig_answer_text))\n",
        "            if actual_text.find(cleaned_answer_text) == -1:\n",
        "              tf.logging.warning(\"Could not find answer: '%s' vs. '%s'\",\n",
        "                                 actual_text, cleaned_answer_text)\n",
        "              continue\n",
        "          else:\n",
        "            start_position = -1\n",
        "            end_position = -1\n",
        "            orig_answer_text = \"\"\n",
        "\n",
        "        example = run_squad.SquadExample(\n",
        "            qas_id=qas_id,\n",
        "            question_text=question_text,\n",
        "            doc_tokens=doc_tokens,\n",
        "            orig_answer_text=orig_answer_text,\n",
        "            start_position=start_position,\n",
        "            end_position=end_position,\n",
        "            is_impossible=is_impossible)\n",
        "        examples.append(example)\n",
        "\n",
        "  return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hm4Lf3LPabd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_examples = read_examples(input_file=input_file, is_training=True)\n",
        "\n",
        "# compute training steps\n",
        "\n",
        "num_train_steps = int(len(train_examples) / batch_size * num_train_epochs)\n",
        "num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
        "\n",
        "# pre-shuffle the input to avoid having to make a very large shuffle buffer in the \"input_fn\"\n",
        "\n",
        "rng = random.Random(12345)\n",
        "rng.shuffle(train_examples)\n",
        "\n",
        "# write to a temporary file to avoid storing very large constant tensors in memory\n",
        "\n",
        "train_writer = run_squad.FeatureWriter(filename=os.path.join(output_dir, \"train.tf_record\"), is_training=True)\n",
        "\n",
        "run_squad.convert_examples_to_features(\n",
        "    examples=train_examples,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=max_seq_length,\n",
        "    doc_stride=doc_stride,\n",
        "    max_query_length=max_query_length,\n",
        "    is_training=True,\n",
        "    output_fn=train_writer.process_feature)\n",
        "\n",
        "train_writer.close()\n",
        "\n",
        "del train_examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRmXQFP0mlIt",
        "colab_type": "text"
      },
      "source": [
        "## Functions to create and build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKGAsm8oaCsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_fn_builder(learning_rate, num_train_steps, num_warmup_steps):\n",
        "  ''' \n",
        "  Creates a model function using the passed parameters for learning_rate, etc.\n",
        "  '''\n",
        "  \n",
        "  def model_fn(features, labels, mode, params):\n",
        "\n",
        "    unique_ids = features[\"unique_ids\"]\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    \n",
        "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "    \n",
        "    if not is_predicting:\n",
        "    \n",
        "      start_positions = features[\"start_positions\"]\n",
        "      end_positions = features[\"end_positions\"]\n",
        "      \n",
        "      (total_loss, start_logits, end_logits) = create_model(\n",
        "          is_predicting=is_predicting, input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, \n",
        "          start_positions=start_positions, end_positions=end_positions)\n",
        "      \n",
        "      train_op = optimization.create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "      \n",
        "      output_spec = tf.estimator.EstimatorSpec(mode=mode, loss=total_loss, train_op=train_op)\n",
        "      \n",
        "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
        "      \n",
        "      start_positions = None\n",
        "      end_positions = None\n",
        "      \n",
        "      (start_logits, end_logits) = create_model(\n",
        "          is_predicting=is_predicting, input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, \n",
        "          start_positions=start_positions, end_positions=end_positions)\n",
        "      \n",
        "      predictions = {\n",
        "          \"unique_ids\": unique_ids,\n",
        "          \"start_logits\": start_logits,\n",
        "          \"end_logits\": end_logits,\n",
        "      }\n",
        "      \n",
        "      output_spec = tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
        "      \n",
        "    else:\n",
        "      raise ValueError(\n",
        "          \"Only TRAIN and PREDICT modes are supported: %s\" % (mode))\n",
        "\n",
        "    return output_spec\n",
        "  \n",
        "  return model_fn\n",
        "\n",
        "def create_model(is_predicting, input_ids, input_mask, segment_ids, start_positions, end_positions):\n",
        "  \"\"\"\n",
        "  Create a BERT model\n",
        "  \"\"\"\n",
        "\n",
        "  bert_module = hub.Module(bert_model_hub, trainable=True)\n",
        "  bert_inputs = dict(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
        "  \n",
        "  output_layer = bert_outputs[\"sequence_output\"]\n",
        "  output_layer_shape = modeling.get_shape_list(output_layer, expected_rank=3)\n",
        "  \n",
        "  batch_size = output_layer_shape[0]\n",
        "  seq_length = output_layer_shape[1]\n",
        "  hidden_size = output_layer_shape[2]\n",
        "\n",
        "  output_weights = tf.get_variable(\"cls/squad/output_weights\", [2, hidden_size], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "  output_bias = tf.get_variable(\"cls/squad/output_bias\", [2], initializer=tf.zeros_initializer())\n",
        "  \n",
        "  with tf.variable_scope(\"total_loss\"):\n",
        "  \n",
        "    # no dropout for question answering\n",
        "\n",
        "    output_layer_matrix = tf.reshape(output_layer, [batch_size * seq_length, hidden_size])\n",
        "  \n",
        "    logits = tf.matmul(output_layer_matrix, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    logits = tf.reshape(logits, [batch_size, seq_length, 2])\n",
        "    logits = tf.transpose(logits, [2, 0, 1])\n",
        "\n",
        "    unstacked_logits = tf.unstack(logits, axis=0)\n",
        "\n",
        "    (start_logits, end_logits) = (unstacked_logits[0], unstacked_logits[1])\n",
        "    \n",
        "    if is_predicting:\n",
        "      return (start_logits, end_logits)\n",
        "  \n",
        "    def compute_loss(logits, positions):\n",
        "      one_hot_positions = tf.one_hot(positions, depth=seq_length, dtype=tf.float32)\n",
        "      log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "      loss = -tf.reduce_mean(tf.reduce_sum(one_hot_positions * log_probs, axis=-1))\n",
        "      return loss\n",
        "  \n",
        "    start_loss = compute_loss(start_logits, start_positions)\n",
        "    end_loss = compute_loss(end_logits, end_positions)\n",
        "    total_loss = (start_loss + end_loss) / 2.0\n",
        "\n",
        "    return (total_loss, start_logits, end_logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFXYmVf6nyBE",
        "colab_type": "text"
      },
      "source": [
        "## Build and  train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wnk2EF4nlqgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pass configuration to the model\n",
        "\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=output_dir,\n",
        "    save_summary_steps=save_summary_steps,\n",
        "    save_checkpoints_steps=save_checkpoints_steps)\n",
        "\n",
        "model_fn = model_fn_builder(\n",
        "    learning_rate=learning_rate,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "  model_fn=model_fn,\n",
        "  config=run_config,\n",
        "  params={\"batch_size\": batch_size})\n",
        "\n",
        "# create an input function for training\n",
        "\n",
        "train_input_fn = run_squad.input_fn_builder(\n",
        "    input_file=train_writer.filename,\n",
        "    seq_length=max_seq_length,\n",
        "    is_training=True,\n",
        "    drop_remainder=False)\n",
        "\n",
        "# train the model\n",
        "\n",
        "current_time = datetime.now()\n",
        "print('Starting fine-tuning of BERT on SQuAD v2...')\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print(\"Fine-tuning took time \", datetime.now() - current_time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxdqAJRAqJZo",
        "colab_type": "text"
      },
      "source": [
        "## Export best model to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiKZMsWTL0Ky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: determine automatically the best model\n",
        "\n",
        "!mv \"/content/output/model.ckpt-xxxxx.data-00000-of-00001\" \"/content/drive/My Drive/BERT SQuAD v2/output/\"\n",
        "!mv \"/content/output/model.ckpt-xxxxx.index\" \"/content/drive/My Drive/BERT SQuAD v2/output/\"\n",
        "!mv \"/content/output/model.ckpt-xxxxx.meta\" \"/content/drive/My Drive/BERT SQuAD v2/output/\"\n",
        "!mv \"/content/output/graph.pbtxt\" \"/content/drive/My Drive/BERT SQuAD v2/output/\"\n",
        "!mv \"/content/output/events.out.tfevents.yyyyyyyyyy.zzzzzzzzzzzz\" \"/content/drive/My Drive/BERT SQuAD v2/output/\"\n",
        "!mv \"/content/output/checkpoint\" \"/content/drive/My Drive/BERT SQuAD v2/output/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQrIR5E8r3cu",
        "colab_type": "text"
      },
      "source": [
        "## Make predictions on SQuAD v2.0 dev set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcHr02-9TrHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_file = os.path.join(root, 'dev-v2.0.json')\n",
        "checkpoint_path = os.path.join(root, 'output/model.ckpt-xxxxx')\n",
        "eval_examples = read_examples(input_file=predict_file, is_training=False)\n",
        "\n",
        "eval_writer = run_squad.FeatureWriter(\n",
        "    filename=os.path.join(output_dir, \"eval.tf_record\"),\n",
        "    is_training=False)\n",
        "eval_features = []\n",
        "\n",
        "def append_feature(feature):\n",
        "    eval_features.append(feature)\n",
        "    eval_writer.process_feature(feature)\n",
        "\n",
        "run_squad.convert_examples_to_features(\n",
        "    examples=eval_examples,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=max_seq_length,\n",
        "    doc_stride=doc_stride,\n",
        "    max_query_length=max_query_length,\n",
        "    is_training=False,\n",
        "    output_fn=append_feature)\n",
        "eval_writer.close()\n",
        "\n",
        "tf.logging.info(\"***** Running predictions *****\")\n",
        "tf.logging.info(\"  Num orig examples = %d\", len(eval_examples))\n",
        "tf.logging.info(\"  Num split examples = %d\", len(eval_features))\n",
        "tf.logging.info(\"  Batch size = %d\", predict_batch_size)\n",
        "\n",
        "predict_input_fn = run_squad.input_fn_builder(\n",
        "    input_file=eval_writer.filename,\n",
        "    seq_length=max_seq_length,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)\n",
        "\n",
        "RawResult = collections.namedtuple(\"RawResult\", [\"unique_id\", \"start_logits\", \"end_logits\"])\n",
        "all_results = []\n",
        "for result in estimator.predict(predict_input_fn, yield_single_examples=True, checkpoint_path=checkpoint_path):\n",
        "  if len(all_results) % 1000 == 0:\n",
        "    tf.logging.info(\"Processing example: %d\" % (len(all_results)))\n",
        "  unique_id = int(result[\"unique_ids\"])\n",
        "  start_logits = [float(x) for x in result[\"start_logits\"].flat]\n",
        "  end_logits = [float(x) for x in result[\"end_logits\"].flat]\n",
        "  all_results.append(RawResult(unique_id=unique_id, start_logits=start_logits, end_logits=end_logits))\n",
        "\n",
        "output_prediction_file = os.path.join(root, \"predictions.json\")\n",
        "output_nbest_file = os.path.join(root, \"nbest_predictions.json\")\n",
        "output_null_log_odds_file = os.path.join(root, \"null_odds.json\")\n",
        "\n",
        "run_squad.write_predictions(eval_examples, eval_features, all_results,\n",
        "                  n_best_size, max_answer_length,\n",
        "                  do_lower_case, output_prediction_file,\n",
        "                  output_nbest_file, output_null_log_odds_file)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}