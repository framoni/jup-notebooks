{"cells":[{"metadata":{"papermill":{"duration":0.020444,"end_time":"2021-02-08T10:52:23.450847","exception":false,"start_time":"2021-02-08T10:52:23.430403","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# [RFCX] Sound Event Detection\n\nBased on the work from [@hidehisaarai1213](https://www.kaggle.com/hidehisaarai1213) and [@gopidurgaprasad](https://www.kaggle.com/gopidurgaprasad)"},{"metadata":{},"cell_type":"markdown","source":"Rainforest Connection (RFCX) Species Audio Detection is a Kaggle competition where the goal is to classify sounds of birds and other animals living in the rainforest. \n\nIt's a sound tagging competition: for each test recording we have to predict the probability that each species is present in it. The goal is to maximize the label-weighted label-ranking average precision (LWLRAP) on the test set.\n\nThis notebook shows how a [Sound Event Detection](https://arxiv.org/abs/1912.10211) model can be used to do the task. After computing the mean prediction over all of the 5 folds one can achieve a LWLRAP score as high as 0.893. "},{"metadata":{"papermill":{"duration":0.018851,"end_time":"2021-02-08T10:52:23.48895","exception":false,"start_time":"2021-02-08T10:52:23.470099","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Install packages"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2021-02-08T10:52:23.539946Z","iopub.status.busy":"2021-02-08T10:52:23.539017Z","iopub.status.idle":"2021-02-08T10:53:00.43979Z","shell.execute_reply":"2021-02-08T10:53:00.439142Z"},"papermill":{"duration":36.932296,"end_time":"2021-02-08T10:53:00.439925","exception":false,"start_time":"2021-02-08T10:52:23.507629","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"!pip -q install --upgrade pip\n!pip -q install audiomentations\n!pip -q install timm\n!pip -q install torchlibrosa","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.019597,"end_time":"2021-02-08T10:53:00.479385","exception":false,"start_time":"2021-02-08T10:53:00.459788","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Import libraries"},{"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-02-08T10:53:00.529022Z","iopub.status.busy":"2021-02-08T10:53:00.527552Z","iopub.status.idle":"2021-02-08T10:53:11.191806Z","shell.execute_reply":"2021-02-08T10:53:11.190601Z"},"papermill":{"duration":10.692527,"end_time":"2021-02-08T10:53:11.191923","exception":false,"start_time":"2021-02-08T10:53:00.499396","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import audiomentations as aa\nfrom functools import partial\nimport numpy as np\nimport os\nimport pandas as pd\nimport random\nfrom sklearn import metrics\nfrom sklearn.model_selection import StratifiedKFold\nimport soundfile as sf\nimport time\nfrom timm.models.efficientnet import tf_efficientnet_b0_ns\nfrom timm.models.resnest import resnest50d\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchlibrosa.stft import Spectrogram, LogmelFilterBank\nfrom torchlibrosa.augmentation import SpecAugmentation\nfrom tqdm import tqdm\nfrom transformers import get_cosine_schedule_with_warmup","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.018846,"end_time":"2021-02-08T10:53:11.230288","exception":false,"start_time":"2021-02-08T10:53:11.211442","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Arguments"},{"metadata":{"execution":{"iopub.execute_input":"2021-02-08T10:53:11.281113Z","iopub.status.busy":"2021-02-08T10:53:11.278968Z","iopub.status.idle":"2021-02-08T10:53:11.283943Z","shell.execute_reply":"2021-02-08T10:53:11.283356Z"},"papermill":{"duration":0.0332,"end_time":"2021-02-08T10:53:11.284046","exception":false,"start_time":"2021-02-08T10:53:11.250846","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class args:\n    \n    batch_size = 16\n    debug = False\n    device = ('cuda' if torch.cuda.is_available() else 'cpu')\n    early_stop = 15\n    epochs = 50\n    epoch_scheduler = False\n    folds = 5\n    lr = 1e-3\n    model_name = \"SED_EFFNETB0\"\n    model_param = {\n    'encoder': 'tf_efficientnet_b0_ns',\n    'sample_rate': 48000,\n    'window_size': 512,\n    'hop_size': 512,\n    'mel_bins': 384,\n    'fmin': 0,\n    'fmax': 24000,\n    'classes_num': 24\n    }\n    num_workers = 4\n    output_dir = \"weights\"\n    pretrain_weights = None\n    seed = 2021\n    start_epoch = 0\n    step_scheduler = True\n    sub_csv = \"../input/rfcx-species-audio-detection/sample_submission.csv\"\n    test_data_path = \"../input/rfcx-species-audio-detection/test\"\n    train_csv = \"train_folds.csv\"\n    train_data_path = \"../input/rfcx-species-audio-detection/train\"   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load data"},{"metadata":{"execution":{"iopub.execute_input":"2021-02-08T10:53:11.377099Z","iopub.status.busy":"2021-02-08T10:53:11.376409Z","iopub.status.idle":"2021-02-08T10:53:11.812057Z","shell.execute_reply":"2021-02-08T10:53:11.811251Z"},"papermill":{"duration":0.469054,"end_time":"2021-02-08T10:53:11.812173","exception":false,"start_time":"2021-02-08T10:53:11.343119","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_tp_df = pd.read_csv(\"../input/rfcx-species-audio-detection/train_tp.csv\").sort_values(\"recording_id\")\nsub_df = pd.read_csv(\"../input/rfcx-species-audio-detection/sample_submission.csv\")\n\ntrain_gby_df = train_tp_df.groupby(\"recording_id\")[[\"species_id\"]].first().reset_index()\ntrain_gby_df = train_gby_df.sample(frac=1, random_state=args.seed).reset_index(drop=True)\ntrain_gby_df.loc[:, 'kfold'] = -1\n\nX = train_gby_df[\"recording_id\"].values\ny = train_gby_df[\"species_id\"].values\n\nkfold = StratifiedKFold(n_splits=args.folds)\nfor fold, (t_idx, v_idx) in enumerate(kfold.split(X, y)):\n    train_gby_df.loc[v_idx, \"kfold\"] = fold\n\ntrain_tp_df = train_tp_df.merge(train_gby_df[['recording_id', 'kfold']], on=\"recording_id\", how=\"left\")\ntrain_tp_df.to_csv(\"train_folds.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.020965,"end_time":"2021-02-08T10:53:11.853186","exception":false,"start_time":"2021-02-08T10:53:11.832221","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Define model architecture"},{"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-02-08T10:53:11.922114Z","iopub.status.busy":"2021-02-08T10:53:11.90593Z","iopub.status.idle":"2021-02-08T10:53:11.946143Z","shell.execute_reply":"2021-02-08T10:53:11.94568Z"},"papermill":{"duration":0.073296,"end_time":"2021-02-08T10:53:11.946256","exception":false,"start_time":"2021-02-08T10:53:11.87296","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def init_layer(layer):\n    nn.init.xavier_uniform_(layer.weight)\n    if hasattr(layer, \"bias\"):\n        if layer.bias is not None:\n            layer.bias.data.fill_(0.)\n\n\ndef init_bn(bn):\n    bn.bias.data.fill_(0.)\n    bn.weight.data.fill_(1.0)\n\n\ndef init_weights(model):\n    classname = model.__class__.__name__\n    if classname.find(\"Conv2d\") != -1:\n        nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n        model.bias.data.fill_(0)\n    elif classname.find(\"BatchNorm\") != -1:\n        model.weight.data.normal_(1.0, 0.02)\n        model.bias.data.fill_(0)\n    elif classname.find(\"GRU\") != -1:\n        for weight in model.parameters():\n            if len(weight.size()) > 1:\n                nn.init.orghogonal_(weight.data)\n    elif classname.find(\"Linear\") != -1:\n        model.weight.data.normal_(0, 0.01)\n        model.bias.data.zero_()\n\n\ndef do_mixup(x: torch.Tensor, mixup_lambda: torch.Tensor):\n    \"\"\"Mixup x of even indexes (0, 2, 4, ...) with x of odd indexes\n    (1, 3, 5, ...).\n    Args:\n      x: (batch_size * 2, ...)\n      mixup_lambda: (batch_size * 2,)\n    Returns:\n      out: (batch_size, ...)\n    \"\"\"\n    out = (x[0::2].transpose(0, -1) * mixup_lambda[0::2] +\n           x[1::2].transpose(0, -1) * mixup_lambda[1::2]).transpose(0, -1)\n    return out\n\n\nclass Mixup(object):\n    def __init__(self, mixup_alpha, random_seed=args.seed):\n        \"\"\"Mixup coefficient generator\"\"\"\n        self.mixup_alpha = mixup_alpha\n        self.random_state = np.random.RandomState(random_seed)\n\n    def get_lambda(self, batch_size):\n        \"\"\"Get mixup random coefficients\n        Args:\n          batch_size: int\n        Returns:\n          mixup_lambdas: (batch_size,)\n        \"\"\"\n        mixup_lambdas = []\n        for n in range(0, batch_size, 2):\n            lam = self.random_state.beta(self.mixup_alpha, self.mixup_alpha, 1)[0]\n            mixup_lambdas.append(lam)\n            mixup_lambdas.append(1. - lam)\n            \n        return torch.from_numpy(np.array(mixup_lambdas, dtype=np.float32))\n\n\ndef interpolate(x: torch.Tensor, ratio: int):\n    \"\"\"Interpolate data in time domain. This is used to compensate the\n    resolution reduction in downsampling of a CNN.\n    Args:\n      x: (batch_size, time_steps, classes_num)\n      ratio: int, ratio to interpolate\n    Returns:\n      upsampled: (batch_size, time_steps * ratio, classes_num)\n    \"\"\"\n    (batch_size, time_steps, classes_num) = x.shape\n    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n    return upsampled\n\n\ndef pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n    is the same as the value of the last frame.\n    Args:\n      framewise_output: (batch_size, frames_num, classes_num)\n      frames_num: int, number of frames to pad\n    Outputs:\n      output: (batch_size, frames_num, classes_num)\n    \"\"\"\n    pad = framewise_output[:, -1:, :].repeat(\n        1, frames_num - framewise_output.shape[1], 1)\n    \"\"\"tensor for padding\"\"\"\n\n    output = torch.cat((framewise_output, pad), dim=1)\n    \"\"\"(batch_size, frames_num, classes_num)\"\"\"\n\n    return output\n\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=(3, 3),\n            stride=(1, 1),\n            padding=(1, 1),\n            bias=False)\n\n        self.conv2 = nn.Conv2d(\n            in_channels=out_channels,\n            out_channels=out_channels,\n            kernel_size=(3, 3),\n            stride=(1, 1),\n            padding=(1, 1),\n            bias=False)\n\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        self.init_weight()\n\n    def init_weight(self):\n        init_layer(self.conv1)\n        init_layer(self.conv2)\n        init_bn(self.bn1)\n        init_bn(self.bn2)\n\n    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n\n        x = input\n        x = F.relu_(self.bn1(self.conv1(x)))\n        x = F.relu_(self.bn2(self.conv2(x)))\n        if pool_type == 'max':\n            x = F.max_pool2d(x, kernel_size=pool_size)\n        elif pool_type == 'avg':\n            x = F.avg_pool2d(x, kernel_size=pool_size)\n        elif pool_type == 'avg+max':\n            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n            x2 = F.max_pool2d(x, kernel_size=pool_size)\n            x = x1 + x2\n        else:\n            raise Exception('Incorrect argument!')\n\n        return x\n\n\nclass AttBlock(nn.Module):\n    def __init__(self, in_features: int, out_features: int, activation=\"linear\", temperature=1.0):\n        super().__init__()\n\n        self.activation = activation\n        self.temperature = temperature\n        self.att = nn.Conv1d(\n            in_channels=in_features,\n            out_channels=out_features,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=True)\n        self.cla = nn.Conv1d(\n            in_channels=in_features,\n            out_channels=out_features,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=True)\n        self.bn_att = nn.BatchNorm1d(out_features)\n        self.init_weights()\n\n    def init_weights(self):\n        init_layer(self.att)\n        init_layer(self.cla)\n        init_bn(self.bn_att)\n\n    def forward(self, x):\n        # x: (n_samples, n_in, n_time)\n        norm_att = torch.softmax(torch.clamp(self.att(x), -10, 10), dim=-1)\n        cla = self.nonlinear_transform(self.cla(x))\n        x = torch.sum(norm_att * cla, dim=2)\n        return x, norm_att, cla\n\n    def nonlinear_transform(self, x):\n        if self.activation == 'linear':\n            return x\n        elif self.activation == 'sigmoid':\n            return torch.sigmoid(x)  ","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-02-08T10:53:12.015192Z","iopub.status.busy":"2021-02-08T10:53:11.999367Z","iopub.status.idle":"2021-02-08T10:53:12.017304Z","shell.execute_reply":"2021-02-08T10:53:12.017787Z"},"papermill":{"duration":0.051853,"end_time":"2021-02-08T10:53:12.01791","exception":false,"start_time":"2021-02-08T10:53:11.966057","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"encoder_params = {\n    \"tf_efficientnet_b0_ns\": {\n        \"features\": 1280,\n        \"init_op\": partial(tf_efficientnet_b0_ns, pretrained=True, drop_path_rate=0.2)\n    },\n    \"resnest50d\": {\n        \"features\": 2048,\n        \"init_op\": partial(resnest50d, pretrained=True)\n    }\n}\n\nclass SEDModel(nn.Module):\n    def __init__(self, encoder, sample_rate, window_size, hop_size, mel_bins, fmin, fmax, classes_num):\n        super().__init__()\n\n        amin = 1e-10\n        center = True\n        pad_mode = 'reflect'\n        ref = 1.0\n        self.interpolate_ratio = 30\n        top_db = None\n        window = 'hann'\n\n        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n            win_length=window_size, window=window, center=center, pad_mode=pad_mode, freeze_parameters=True)\n        \n        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, freeze_parameters=True)\n\n        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, freq_drop_width=8, freq_stripes_num=2)\n        \n        self.encoder = encoder_params[encoder][\"init_op\"]()\n        self.fc = nn.Linear(encoder_params[encoder][\"features\"], 1024, bias=True)\n        self.att_block = AttBlock(1024, classes_num, activation=\"sigmoid\")\n        self.bn = nn.BatchNorm2d(mel_bins)\n        self.init_weight()\n    \n    def init_weight(self):\n        init_layer(self.fc)\n        init_bn(self.bn)\n    \n    def forward(self, input, mixup_lambda=None):\n        x = self.spectrogram_extractor(input) # batch_size x 1 x time_steps x freq_bins\n        x = self.logmel_extractor(x) # batch_size x 1 x time_steps x mel_bins\n\n        frames_num = x.shape[2]\n\n        x = x.transpose(1, 3)\n        x = self.bn(x)\n        x = x.transpose(1, 3)\n\n        if self.training:\n            x = self.spec_augmenter(x)\n        \n        if self.training and mixup_lambda is not None:\n            x = do_mixup(x, mixup_lambda)\n        \n        x = x.expand(x.shape[0], 3, x.shape[2], x.shape[3]) # batch_size x 3 x time_steps x mel_bins\n        x = self.encoder.forward_features(x)\n        x = torch.mean(x, dim=3)\n\n        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n        x = x1 + x2\n\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = x.transpose(1, 2)\n        x = F.relu_(self.fc(x))\n        x = x.transpose(1, 2)\n        x = F.dropout(x, p=0.5, training=self.training)\n\n        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n        segmentwise_output = segmentwise_output.transpose(1, 2)\n\n        framewise_output = interpolate(segmentwise_output, self.interpolate_ratio)\n        framewise_output = pad_framewise_output(framewise_output, frames_num)\n\n        output_dict = {\n            'framewise_output': framewise_output,\n            'logit': logit,\n            'clipwise_output': clipwise_output\n        }\n\n        return output_dict","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.022613,"end_time":"2021-02-08T10:53:12.061809","exception":false,"start_time":"2021-02-08T10:53:12.039196","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Define dataset"},{"metadata":{"execution":{"iopub.execute_input":"2021-02-08T10:53:12.167624Z","iopub.status.busy":"2021-02-08T10:53:12.166729Z","iopub.status.idle":"2021-02-08T10:53:12.170155Z","shell.execute_reply":"2021-02-08T10:53:12.169604Z"},"papermill":{"duration":0.038114,"end_time":"2021-02-08T10:53:12.170256","exception":false,"start_time":"2021-02-08T10:53:12.132142","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def crop_or_pad(y, sr, period, record):\n    sample_length = len(y)\n    win_length = sr * period\n    rint = np.random.randint(len(record['t_min']))\n    time_start = record['t_min'][rint] * sr\n    time_end = record['t_max'][rint] * sr\n    if sample_length > win_length:\n        c1 = time_end - win_length / 2\n        c2 = time_start + win_length / 2\n        if c1 < win_length / 2:\n            c1 = win_length / 2\n        if c2 > sample_length - win_length / 2:\n            c2 = sample_length - win_length / 2\n        center = np.random.randint(c1, c2)\n        beginning = int(center - win_length / 2)\n        ending = int(center + win_length / 2)\n        y = y[beginning:ending].astype(np.float32)\n    else:\n        y = y.astype(np.float32)\n        beginning = 0\n        ending = win_length\n    beginning_time = beginning / sr\n    ending_time = ending / sr\n    label = np.zeros(24, dtype='f')\n    for i in range(len(record['t_min'])):\n        if (record['t_min'][i] <= ending_time) and (record['t_max'][i] >= beginning_time):\n            label[record['species_id'][i]] = 1\n    return y, label","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-02-08T10:53:12.226314Z","iopub.status.busy":"2021-02-08T10:53:12.224474Z","iopub.status.idle":"2021-02-08T10:53:12.228996Z","shell.execute_reply":"2021-02-08T10:53:12.228476Z"},"papermill":{"duration":0.037748,"end_time":"2021-02-08T10:53:12.229091","exception":false,"start_time":"2021-02-08T10:53:12.191343","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class SEDDataset:\n    \n    def __init__(self, df, period=10, stride=10, audio_transform=None, data_path=None, mode=None):\n        self.df = df.groupby(\"recording_id\").agg(lambda x: list(x)).reset_index()\n        if mode == 'train':\n            self.df = self.df.append(self.df).reset_index(drop=True)\n        self.period = period\n        self.stride = stride\n        self.audio_transform = audio_transform\n        self.data_path = data_path\n        self.mode = mode\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        record = self.df.iloc[idx]\n        y, sr = sf.read(f\"{self.data_path}/{record['recording_id']}.flac\")\n        \n        if self.mode != \"test\":\n            y, label = crop_or_pad(y, sr, period=self.period, record=record)\n            if self.audio_transform:\n                y = self.audio_transform(samples=y, sample_rate=sr)\n        else:\n            win_length = self.period * sr\n            stride = self.stride * sr\n            y = np.stack([y[i:i+win_length].astype(np.float32) for i in range(0, 60*sr+stride-win_length, stride)])\n            label = np.zeros(24, dtype='f')\n            \n        return {\n            \"image\": y,\n            \"target\": label,\n            \"id\": record['recording_id']\n        }","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.020498,"end_time":"2021-02-08T10:53:12.270576","exception":false,"start_time":"2021-02-08T10:53:12.250078","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Augmentations"},{"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-02-08T10:53:12.320951Z","iopub.status.busy":"2021-02-08T10:53:12.320196Z","iopub.status.idle":"2021-02-08T10:53:12.324363Z","shell.execute_reply":"2021-02-08T10:53:12.323813Z"},"papermill":{"duration":0.032037,"end_time":"2021-02-08T10:53:12.324457","exception":false,"start_time":"2021-02-08T10:53:12.29242","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_audio_transform = aa.Compose([\n    aa.AddGaussianNoise(p=0.5),\n    aa.AddGaussianSNR(p=0.5),\n    aa.Shift(p=0.5),\n    aa.Gain(p=0.5),\n    aa.AddBackgroundNoise(sounds_path=\"/kaggle/input/birdcall-noise-resample/\")\n])","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.020702,"end_time":"2021-02-08T10:53:12.366667","exception":false,"start_time":"2021-02-08T10:53:12.345965","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Utility functions"},{"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-02-08T10:53:12.427353Z","iopub.status.busy":"2021-02-08T10:53:12.425472Z","iopub.status.idle":"2021-02-08T10:53:12.4281Z","shell.execute_reply":"2021-02-08T10:53:12.428658Z"},"papermill":{"duration":0.041475,"end_time":"2021-02-08T10:53:12.428765","exception":false,"start_time":"2021-02-08T10:53:12.38729","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def _lwlrap_sklearn(truth, scores):\n    \"\"\"Reference implementation from https://colab.research.google.com/drive/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8\"\"\"\n    sample_weight = np.sum(truth > 0, axis=1)\n    nonzero_weight_sample_indices = np.flatnonzero(sample_weight > 0)\n    overall_lwlrap = metrics.label_ranking_average_precision_score(\n        truth[nonzero_weight_sample_indices, :] > 0, \n        scores[nonzero_weight_sample_indices, :], \n        sample_weight=sample_weight[nonzero_weight_sample_indices])\n    return overall_lwlrap\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\nclass MetricMeter(object):\n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.y_true = []\n        self.y_pred = []\n    \n    def update(self, y_true, y_pred):\n        self.y_true.extend(y_true.cpu().detach().numpy().tolist())\n        self.y_pred.extend(y_pred.cpu().detach().numpy().tolist())\n\n    @property\n    def avg(self):\n        self.score = _lwlrap_sklearn(np.array(self.y_true), np.array(self.y_pred))\n        return {\n            \"lwlrap\" : self.score\n        }\n\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.021072,"end_time":"2021-02-08T10:53:12.470919","exception":false,"start_time":"2021-02-08T10:53:12.449847","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Losses"},{"metadata":{"execution":{"iopub.execute_input":"2021-02-08T10:53:12.521622Z","iopub.status.busy":"2021-02-08T10:53:12.520947Z","iopub.status.idle":"2021-02-08T10:53:12.524853Z","shell.execute_reply":"2021-02-08T10:53:12.525343Z"},"papermill":{"duration":0.033387,"end_time":"2021-02-08T10:53:12.525458","exception":false,"start_time":"2021-02-08T10:53:12.492071","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class PANNsLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.bce = nn.BCELoss()\n\n    def forward(self, input, target):\n        input_ = input[\"clipwise_output\"]\n        input_ = torch.where(torch.isnan(input_), torch.zeros_like(input_), input_)\n        input_ = torch.where(torch.isinf(input_), torch.zeros_like(input_), input_)\n        target = target.float()\n\n        return self.bce(input_, target)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.020812,"end_time":"2021-02-08T10:53:12.617988","exception":false,"start_time":"2021-02-08T10:53:12.597176","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Train function"},{"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-02-08T10:53:12.686619Z","iopub.status.busy":"2021-02-08T10:53:12.674212Z","iopub.status.idle":"2021-02-08T10:53:12.689417Z","shell.execute_reply":"2021-02-08T10:53:12.688934Z"},"papermill":{"duration":0.049812,"end_time":"2021-02-08T10:53:12.689529","exception":false,"start_time":"2021-02-08T10:53:12.639717","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def train_epoch(args, model, loader, criterion, optimizer, scheduler, epoch):\n    losses = AverageMeter()\n    scores = MetricMeter()\n    model.train()\n    t = tqdm(loader)\n    for i, sample in enumerate(t):\n        optimizer.zero_grad()\n        input = sample['image'].to(args.device)\n        target = sample['target'].to(args.device)\n        M = Mixup(.4)\n        lam = M.get_lambda(args.batch_size).to(args.device)\n        target = do_mixup(target, lam)\n        output = model(input, mixup_lambda=lam)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        if scheduler and args.step_scheduler:\n            scheduler.step()\n        bs = input.size(0)\n        scores.update(target, torch.sigmoid(torch.max(output['framewise_output'], dim=1)[0]))\n        losses.update(loss.item(), bs)\n        t.set_description(f\"Train E: {epoch} - Loss: {losses.avg:0.4f}\")\n    t.close()\n    return scores.avg, losses.avg\n        \ndef valid_epoch(args, model, loader, criterion, epoch):\n    losses = AverageMeter()\n    scores = MetricMeter()\n    model.eval()\n    with torch.no_grad():\n        t = tqdm(loader)\n        for i, sample in enumerate(t):\n            input = sample['image'].to(args.device)\n            target = sample['target'].to(args.device)\n            output = model(input)\n            loss = criterion(output, target)\n            bs = input.size(0)\n            scores.update(target, torch.sigmoid(torch.max(output['framewise_output'], dim=1)[0]))\n            losses.update(loss.item(), bs)\n            t.set_description(f\"Valid E: {epoch} - Loss: {losses.avg:0.4f}\")\n    t.close()\n    return scores.avg, losses.avg\n\ndef test_epoch(args, model, loader):\n    model.eval()\n    pred_list = []\n    id_list = []\n    with torch.no_grad():\n        t = tqdm(loader)\n        for i, sample in enumerate(t):\n            input = sample[\"image\"].to(args.device)\n            bs, seq, w = input.shape\n            input = input.reshape(bs*seq, w)\n            id = sample[\"id\"]\n            output = model(input)\n            output = torch.sigmoid(torch.max(output['framewise_output'], dim=1)[0])\n            output = output.reshape(bs, seq, -1)\n            output = torch.sum(output, dim=1)\n            output = output.cpu().detach().numpy().tolist()\n            pred_list.extend(output)\n            id_list.extend(id)\n    return pred_list, id_list","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.020957,"end_time":"2021-02-08T10:53:12.767285","exception":false,"start_time":"2021-02-08T10:53:12.746328","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Main function"},{"metadata":{"execution":{"iopub.execute_input":"2021-02-08T10:53:12.872688Z","iopub.status.busy":"2021-02-08T10:53:12.871Z","iopub.status.idle":"2021-02-08T10:53:12.925538Z","shell.execute_reply":"2021-02-08T10:53:12.926866Z"},"papermill":{"duration":0.088921,"end_time":"2021-02-08T10:53:12.927037","exception":false,"start_time":"2021-02-08T10:53:12.838116","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def main(fold):\n    \n    seed_everything(args.seed)\n\n    args.fold = fold\n    \n    args.save_path = os.path.join(args.output_dir, args.model_name)\n    os.makedirs(args.save_path, exist_ok=True)\n\n    train_df = pd.read_csv(args.train_csv)\n    sub_df = pd.read_csv(args.sub_csv)\n    if args.debug:\n        train_df = train_df.sample(200)\n    train_fold_df = train_df[train_df.kfold != fold]\n    valid_fold_df = train_df[train_df.kfold == fold]\n    \n    train_dataset = SEDDataset(\n        df=train_fold_df,\n        audio_transform=train_audio_transform,\n        data_path=args.train_data_path,\n        mode=\"train\")\n    valid_dataset = SEDDataset(\n        df=valid_fold_df,\n        audio_transform=None,\n        data_path=args.train_data_path,\n        mode=\"valid\")\n    test_dataset = SEDDataset(\n        df=sub_df,\n        audio_transform=None,\n        data_path=args.test_data_path,\n        mode=\"test\")\n    \n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=args.batch_size,\n        shuffle=True,\n        drop_last=True,\n        num_workers=args.num_workers)\n    valid_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=args.batch_size,\n        shuffle=False,\n        drop_last=False,\n        num_workers=args.num_workers)\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=args.batch_size,\n        shuffle=False,\n        drop_last=False,\n        num_workers=args.num_workers)\n\n    model = SEDModel(**args.model_param)\n    model = model.to(args.device)\n\n    if args.pretrain_weights:\n        print(\"Loading pretrained weights...\")\n        model.load_state_dict(torch.load(args.pretrain_weights, map_location=args.device), strict=False)\n        model = model.to(args.device)\n    else:\n        criterion = PANNsLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n        num_train_steps = int(len(train_loader) * args.epochs)\n        num_warmup_steps = int(0.1 * args.epochs * len(train_loader))\n        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n\n        best_lwlrap = -np.inf\n        early_stop_count = 0\n\n        for epoch in range(args.start_epoch, args.epochs):\n            train_avg, train_loss = train_epoch(args, model, train_loader, criterion, optimizer, scheduler, epoch)\n            valid_avg, valid_loss = valid_epoch(args, model, valid_loader, criterion, epoch)\n\n            if args.epoch_scheduler:\n                scheduler.step()\n\n            content = f\"\"\"\n                      {time.ctime()} \\n\n                      Fold: {args.fold}, Epoch: {epoch}, lr: {optimizer.param_groups[0]['lr']:.7}\\n\n                      Train Loss: {train_loss:0.4f} - LWLRAP: {train_avg['lwlrap']:0.4f}\\n\n                      Valid Loss: {valid_loss:0.4f} - LWLRAP: {valid_avg['lwlrap']:0.4f}\\n\n                      \"\"\"\n            print(content)\n\n            with open(f'{args.save_path}/log_{args.model_name}.txt', 'a') as appender:\n                appender.write(content + '\\n')\n\n            if valid_avg['lwlrap'] > best_lwlrap:\n                print(f\"Validation LWLRAP improved from {best_lwlrap} to {valid_avg['lwlrap']}\")\n                torch.save(model.state_dict(), os.path.join(args.save_path, f'fold-{args.fold}.bin'))\n                best_lwlrap = valid_avg['lwlrap']\n                early_stop_count = 0\n            else:\n                early_stop_count += 1\n            # torch.save(model.state_dict(), os.path.join(args.save_path, f'fold-{args.fold}_last.bin'))\n\n            if args.early_stop == early_stop_count:\n                print(\"Early stopping count reached: \", early_stop_count)\n                break\n\n        model.load_state_dict(torch.load(os.path.join(args.save_path, f'fold-{args.fold}.bin'), map_location=args.device))\n        model = model.to(args.device)\n\n    target_cols = sub_df.columns[1:].values.tolist()\n    test_pred, ids = test_epoch(args, model, test_loader)\n    print(np.array(test_pred).shape)\n\n    test_pred_df = pd.DataFrame({\n        \"recording_id\" : sub_df.recording_id.values\n    })\n    test_pred_df[target_cols] = test_pred\n    test_pred_df.to_csv(os.path.join(args.save_path, f\"fold-{args.fold}-submission.csv\"), index=False)\n    print(os.path.join(args.save_path, f\"fold-{args.fold}-submission.csv\"))\n    ","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.036344,"end_time":"2021-02-08T10:53:13.00291","exception":false,"start_time":"2021-02-08T10:53:12.966566","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Train folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"main(fold=0)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}