{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT SQuAD v2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/framoni/jup-notebooks/blob/master/BERT_SQuAD_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJNKA5PWzhqs",
        "colab_type": "text"
      },
      "source": [
        "# Fine-tuning BERT on SQuAD v2 dataset for question answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeRNFLFr4ehq",
        "colab_type": "text"
      },
      "source": [
        "This notebook reproduces from scratch and without concerns the fine-tuning of BERT on the SQuAD v2 dataset for question answering tasks using the models uploaded on TensorFlow Hub. Just run the notebook and don't worry about details!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj2worJVzTW4",
        "colab_type": "text"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mshLHDcwSe0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install bert-tensorflow\n",
        "\n",
        "import bert\n",
        "from bert import modeling\n",
        "from bert import optimization\n",
        "from bert import run_squad\n",
        "from bert import tokenization\n",
        "import collections\n",
        "import datetime\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import six\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PsTb6lGF1h0",
        "colab_type": "text"
      },
      "source": [
        "## Set root folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRnnvhA5LjOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if use_drive = False, upload SQuAD \"train-v2.0.json\" \"dev-v2.0.json\" on Colab\n",
        "# if use_drive = True, upload those files in a folder named \"BERT SQuAD\" in your Google Drive\n",
        "\n",
        "use_drive = True\n",
        "\n",
        "if use_drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  root  = '/content/drive/My Drive/BERT SQuAD v2'\n",
        "else:\n",
        "  root = ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdNJRSLjf1Pr",
        "colab_type": "text"
      },
      "source": [
        "## Function to read samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2PqtNcZSnZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_examples(input_file, is_training):\n",
        "  \"\"\"\n",
        "  This function is an extension of run_squad.read_squad_examples to allow for input of external text files as paragraphs\n",
        "  \"\"\"\n",
        "\n",
        "  with tf.gfile.Open(input_file, \"r\") as reader:\n",
        "    input_data = json.load(reader)[\"data\"]\n",
        "\n",
        "  def is_whitespace(c):\n",
        "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  examples = []\n",
        "  for entry in input_data:\n",
        "    for paragraph in entry[\"paragraphs\"]:\n",
        "      paragraph_text = paragraph[\"context\"]\n",
        "      if os.path.isfile(paragraph_text):\n",
        "            with open(paragraph_text, \"r\", encoding=\"utf-8\") as f:\n",
        "                paragraph_text = f.read()\n",
        "      doc_tokens = []\n",
        "      char_to_word_offset = []\n",
        "      prev_is_whitespace = True\n",
        "      for c in paragraph_text:\n",
        "        if is_whitespace(c):\n",
        "          prev_is_whitespace = True\n",
        "        else:\n",
        "          if prev_is_whitespace:\n",
        "            doc_tokens.append(c)\n",
        "          else:\n",
        "            doc_tokens[-1] += c\n",
        "          prev_is_whitespace = False\n",
        "        char_to_word_offset.append(len(doc_tokens) - 1)\n",
        "      for qa in paragraph[\"qas\"]:\n",
        "        qas_id = qa[\"id\"]\n",
        "        question_text = qa[\"question\"]\n",
        "        start_position = None\n",
        "        end_position = None\n",
        "        orig_answer_text = None\n",
        "        is_impossible = False\n",
        "        if is_training:\n",
        "          is_impossible = qa[\"is_impossible\"]\n",
        "          if (len(qa[\"answers\"]) != 1) and (not is_impossible):\n",
        "            raise ValueError(\n",
        "                \"For training, each question should have exactly 1 answer.\")\n",
        "          if not is_impossible:\n",
        "            answer = qa[\"answers\"][0]\n",
        "            orig_answer_text = answer[\"text\"]\n",
        "            answer_offset = answer[\"answer_start\"]\n",
        "            answer_length = len(orig_answer_text)\n",
        "            start_position = char_to_word_offset[answer_offset]\n",
        "            end_position = char_to_word_offset[answer_offset + answer_length - 1]\n",
        "            actual_text = \" \".join(\n",
        "                doc_tokens[start_position:(end_position + 1)])\n",
        "            cleaned_answer_text = \" \".join(\n",
        "                tokenization.whitespace_tokenize(orig_answer_text))\n",
        "            if actual_text.find(cleaned_answer_text) == -1:\n",
        "              tf.logging.warning(\"Could not find answer: '%s' vs. '%s'\",\n",
        "                                 actual_text, cleaned_answer_text)\n",
        "              continue\n",
        "          else:\n",
        "            start_position = -1\n",
        "            end_position = -1\n",
        "            orig_answer_text = \"\"\n",
        "\n",
        "        example = run_squad.SquadExample(\n",
        "            qas_id=qas_id,\n",
        "            question_text=question_text,\n",
        "            doc_tokens=doc_tokens,\n",
        "            orig_answer_text=orig_answer_text,\n",
        "            start_position=start_position,\n",
        "            end_position=end_position,\n",
        "            is_impossible=is_impossible)\n",
        "        examples.append(example)\n",
        "\n",
        "  return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA2ktuPTmbar",
        "colab_type": "text"
      },
      "source": [
        "## Settings and model parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HicikOcdmYp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BERT-large\n",
        "\n",
        "# bert_model_hub = 'https://tfhub.dev/google/bert_uncased_L-24_H-1024_A-16/1'\n",
        "\n",
        "# BERT-small\n",
        "\n",
        "bert_model_hub = 'https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1'\n",
        "\n",
        "output_dir = 'output'\n",
        "do_delete = True # whether do delete the content of output folder if it already exists\n",
        "input_file = os.path.join(root, 'train-v2.0.json')\n",
        "\n",
        "batch_size = 32\n",
        "predict_batch_size = 8\n",
        "learning_rate = 5e-5\n",
        "num_train_epochs = 3.0\n",
        "warmup_proportion = 0.1\n",
        "\n",
        "max_seq_length = 128\n",
        "max_query_length = 64\n",
        "max_answer_length = 30\n",
        "\n",
        "doc_stride = 128 # stride of the window sliding through the length of the document\n",
        "null_score_diff_threshold = 0.0 # threshold on the difference (null_score - best_non_null) over which to predict null\n",
        "n_best_size = 3 # number of n-best predictions to generate in the nbest_predictions.json output file\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.WARN) # set TF verbosity\n",
        "\n",
        "save_checkpoints_steps = 1000\n",
        "save_summary_steps = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WBHcDF9LY0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_examples = read_examples(input_file=input_file, is_training=True)\n",
        "\n",
        "# compute training steps\n",
        "\n",
        "num_train_steps = int(len(train_examples) / batch_size * num_train_epochs)\n",
        "num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
        "\n",
        "del train_examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKrdESZgmmw4",
        "colab_type": "text"
      },
      "source": [
        "## Create output folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXahxo266Ra9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if do_delete:\n",
        "  try:\n",
        "    tf.gfile.DeleteRecursively(output_dir)\n",
        "  except:\n",
        "    pass\n",
        "tf.gfile.MakeDirs(output_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5ia0wkqncWm",
        "colab_type": "text"
      },
      "source": [
        "## Create BERT tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW7Nk0f0nyVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"\n",
        "  Get the vocab file and casing info from the Hub module\n",
        "  \"\"\"\n",
        "  \n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(bert_model_hub)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"], tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return [do_lower_case, bert.tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)]\n",
        "\n",
        "[do_lower_case, tokenizer] = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W8BrfcAm7Ot",
        "colab_type": "text"
      },
      "source": [
        "## Load training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hm4Lf3LPabd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_examples = read_examples(input_file=input_file, is_training=True)\n",
        "\n",
        "# pre-shuffle the input to avoid having to make a very large shuffle buffer in the \"input_fn\"\n",
        "\n",
        "rng = random.Random(12345)\n",
        "rng.shuffle(train_examples)\n",
        "\n",
        "# write to a temporary file to avoid storing very large constant tensors in memory\n",
        "\n",
        "train_writer = run_squad.FeatureWriter(filename=os.path.join(output_dir, \"train.tf_record\"), is_training=True)\n",
        "\n",
        "run_squad.convert_examples_to_features(\n",
        "    examples=train_examples,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=max_seq_length,\n",
        "    doc_stride=doc_stride,\n",
        "    max_query_length=max_query_length,\n",
        "    is_training=True,\n",
        "    output_fn=train_writer.process_feature)\n",
        "\n",
        "train_writer.close()\n",
        "\n",
        "del train_examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRmXQFP0mlIt",
        "colab_type": "text"
      },
      "source": [
        "## Functions to create and build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKGAsm8oaCsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_fn_builder(learning_rate, num_train_steps, num_warmup_steps):\n",
        "  ''' \n",
        "  Creates a model function using the passed parameters for learning_rate, etc.\n",
        "  '''\n",
        "  \n",
        "  def model_fn(features, labels, mode, params):\n",
        "\n",
        "    unique_ids = features[\"unique_ids\"]\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    \n",
        "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "    \n",
        "    if not is_predicting:\n",
        "    \n",
        "      start_positions = features[\"start_positions\"]\n",
        "      end_positions = features[\"end_positions\"]\n",
        "      \n",
        "      (total_loss, start_logits, end_logits) = create_model(\n",
        "          is_predicting=is_predicting, input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, \n",
        "          start_positions=start_positions, end_positions=end_positions)\n",
        "      \n",
        "      train_op = optimization.create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "      \n",
        "      output_spec = tf.estimator.EstimatorSpec(mode=mode, loss=total_loss, train_op=train_op)\n",
        "      \n",
        "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
        "      \n",
        "      start_positions = None\n",
        "      end_positions = None\n",
        "      \n",
        "      (start_logits, end_logits) = create_model(\n",
        "          is_predicting=is_predicting, input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, \n",
        "          start_positions=start_positions, end_positions=end_positions)\n",
        "      \n",
        "      predictions = {\n",
        "          \"unique_ids\": unique_ids,\n",
        "          \"start_logits\": start_logits,\n",
        "          \"end_logits\": end_logits,\n",
        "      }\n",
        "      \n",
        "      output_spec = tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
        "      \n",
        "    else:\n",
        "      raise ValueError(\n",
        "          \"Only TRAIN and PREDICT modes are supported: %s\" % (mode))\n",
        "\n",
        "    return output_spec\n",
        "  \n",
        "  return model_fn\n",
        "\n",
        "def create_model(is_predicting, input_ids, input_mask, segment_ids, start_positions, end_positions):\n",
        "  \"\"\"\n",
        "  Create a BERT model\n",
        "  \"\"\"\n",
        "\n",
        "  bert_module = hub.Module(bert_model_hub, trainable=True)\n",
        "  bert_inputs = dict(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
        "  \n",
        "  output_layer = bert_outputs[\"sequence_output\"]\n",
        "  output_layer_shape = modeling.get_shape_list(output_layer, expected_rank=3)\n",
        "  \n",
        "  batch_size = output_layer_shape[0]\n",
        "  seq_length = output_layer_shape[1]\n",
        "  hidden_size = output_layer_shape[2]\n",
        "\n",
        "  output_weights = tf.get_variable(\"cls/squad/output_weights\", [2, hidden_size], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "  output_bias = tf.get_variable(\"cls/squad/output_bias\", [2], initializer=tf.zeros_initializer())\n",
        "  \n",
        "  with tf.variable_scope(\"total_loss\"):\n",
        "  \n",
        "    # no dropout for question answering\n",
        "\n",
        "    output_layer_matrix = tf.reshape(output_layer, [batch_size * seq_length, hidden_size])\n",
        "  \n",
        "    logits = tf.matmul(output_layer_matrix, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    logits = tf.reshape(logits, [batch_size, seq_length, 2])\n",
        "    logits = tf.transpose(logits, [2, 0, 1])\n",
        "\n",
        "    unstacked_logits = tf.unstack(logits, axis=0)\n",
        "\n",
        "    (start_logits, end_logits) = (unstacked_logits[0], unstacked_logits[1])\n",
        "    \n",
        "    if is_predicting:\n",
        "      return (start_logits, end_logits)\n",
        "  \n",
        "    def compute_loss(logits, positions):\n",
        "      one_hot_positions = tf.one_hot(positions, depth=seq_length, dtype=tf.float32)\n",
        "      log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "      loss = -tf.reduce_mean(tf.reduce_sum(one_hot_positions * log_probs, axis=-1))\n",
        "      return loss\n",
        "  \n",
        "    start_loss = compute_loss(start_logits, start_positions)\n",
        "    end_loss = compute_loss(end_logits, end_positions)\n",
        "    total_loss = (start_loss + end_loss) / 2.0\n",
        "\n",
        "    return (total_loss, start_logits, end_logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFXYmVf6nyBE",
        "colab_type": "text"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wnk2EF4nlqgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pass configuration to the model\n",
        "\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=output_dir,\n",
        "    save_summary_steps=save_summary_steps,\n",
        "    save_checkpoints_steps=save_checkpoints_steps)\n",
        "\n",
        "model_fn = model_fn_builder(\n",
        "    learning_rate=learning_rate,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "  model_fn=model_fn,\n",
        "  config=run_config,\n",
        "  params={\"batch_size\": batch_size})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru3rP0fiO-pI",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdfGQPxuPCOR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create an input function for training\n",
        "\n",
        "train_input_fn = run_squad.input_fn_builder(\n",
        "    input_file=train_writer.filename,\n",
        "    seq_length=max_seq_length,\n",
        "    is_training=True,\n",
        "    drop_remainder=False)\n",
        "\n",
        "# train the model\n",
        "\n",
        "current_time = datetime.datetime.now()\n",
        "print('Starting fine-tuning of BERT on SQuAD v2...')\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print(\"Fine-tuning took time \", datetime.datetime.now() - current_time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxdqAJRAqJZo",
        "colab_type": "text"
      },
      "source": [
        "## Export best model to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiKZMsWTL0Ky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: determine automatically the best model\n",
        "\n",
        "!mv \"/content/output/model.ckpt-xxxxx.data-00000-of-00001\" \"/content/drive/My Drive/BERT SQuAD v2/output/\"\n",
        "!mv \"/content/output/model.ckpt-xxxxx.index\" \"/content/drive/My Drive/BERT SQuAD v2/output/\"\n",
        "!mv \"/content/output/model.ckpt-xxxxx.meta\" \"/content/drive/My Drive/BERT SQuAD v2/output/\"\n",
        "!mv \"/content/output/graph.pbtxt\" \"/content/drive/My Drive/BERT SQuAD v2/output/\"\n",
        "!mv \"/content/output/events.out.tfevents.yyyyyyyyyy.zzzzzzzzzzzz\" \"/content/drive/My Drive/BERT SQuAD v2/output/\"\n",
        "!mv \"/content/output/checkpoint\" \"/content/drive/My Drive/BERT SQuAD v2/output/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge9_2fmEOwnA",
        "colab_type": "text"
      },
      "source": [
        "## Function to write predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjVtmobnOlB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_predictions(all_examples, all_features, all_results, n_best_size,\n",
        "                      max_answer_length, do_lower_case, output_prediction_file,\n",
        "                      output_nbest_file, output_null_log_odds_file):\n",
        "  \"\"\"\n",
        "  Write final predictions to the json file and log-odds of null if needed\n",
        "  \"\"\"\n",
        "  \n",
        "  tf.logging.info(\"Writing predictions to: %s\" % (output_prediction_file))\n",
        "  tf.logging.info(\"Writing nbest to: %s\" % (output_nbest_file))\n",
        "\n",
        "  example_index_to_features = collections.defaultdict(list)\n",
        "  for feature in all_features:\n",
        "    example_index_to_features[feature.example_index].append(feature)\n",
        "\n",
        "  unique_id_to_result = {}\n",
        "  for result in all_results:\n",
        "    unique_id_to_result[result.unique_id] = result\n",
        "\n",
        "  _PrelimPrediction = collections.namedtuple(\n",
        "      \"PrelimPrediction\", [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n",
        "\n",
        "  all_predictions = collections.OrderedDict()\n",
        "  all_nbest_json = collections.OrderedDict()\n",
        "  scores_diff_json = collections.OrderedDict()\n",
        "\n",
        "  for (example_index, example) in enumerate(all_examples):\n",
        "    features = example_index_to_features[example_index]\n",
        "\n",
        "    prelim_predictions = []\n",
        "    # keep track of the minimum score of null start+end of position 0\n",
        "    score_null = 1000000  # large and positive\n",
        "    min_null_feature_index = 0  # the paragraph slice with min mull score\n",
        "    null_start_logit = 0  # the start logit at the slice with min null score\n",
        "    null_end_logit = 0  # the end logit at the slice with min null score\n",
        "    for (feature_index, feature) in enumerate(features):\n",
        "      result = unique_id_to_result[feature.unique_id]\n",
        "      start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
        "      end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
        "      # if we could have irrelevant answers, get the min score of irrelevant\n",
        "      feature_null_score = result.start_logits[0] + result.end_logits[0]\n",
        "      if feature_null_score < score_null:\n",
        "        score_null = feature_null_score\n",
        "        min_null_feature_index = feature_index\n",
        "        null_start_logit = result.start_logits[0]\n",
        "        null_end_logit = result.end_logits[0]\n",
        "      for start_index in start_indexes:\n",
        "        for end_index in end_indexes:\n",
        "          if start_index >= len(feature.tokens):\n",
        "            continue\n",
        "          if end_index >= len(feature.tokens):\n",
        "            continue\n",
        "          if start_index not in feature.token_to_orig_map:\n",
        "            continue\n",
        "          if end_index not in feature.token_to_orig_map:\n",
        "            continue\n",
        "          if not feature.token_is_max_context.get(start_index, False):\n",
        "            continue\n",
        "          if end_index < start_index:\n",
        "            continue\n",
        "          length = end_index - start_index + 1\n",
        "          if length > max_answer_length:\n",
        "            continue\n",
        "          prelim_predictions.append(\n",
        "              _PrelimPrediction(\n",
        "                  feature_index=feature_index,\n",
        "                  start_index=start_index,\n",
        "                  end_index=end_index,\n",
        "                  start_logit=result.start_logits[start_index],\n",
        "                  end_logit=result.end_logits[end_index]))\n",
        "\n",
        "    prelim_predictions.append(\n",
        "        _PrelimPrediction(\n",
        "            feature_index=min_null_feature_index,\n",
        "            start_index=0,\n",
        "            end_index=0,\n",
        "            start_logit=null_start_logit,\n",
        "            end_logit=null_end_logit))\n",
        "    prelim_predictions = sorted(\n",
        "        prelim_predictions,\n",
        "        key=lambda x: (x.start_logit + x.end_logit),\n",
        "        reverse=True)\n",
        "\n",
        "    _NbestPrediction = collections.namedtuple(\n",
        "        \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"])\n",
        "\n",
        "    seen_predictions = {}\n",
        "    nbest = []\n",
        "    for pred in prelim_predictions:\n",
        "      if len(nbest) >= n_best_size:\n",
        "        break\n",
        "      feature = features[pred.feature_index]\n",
        "      if pred.start_index > 0:  # this is a non-null prediction\n",
        "        tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n",
        "        orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
        "        orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
        "        orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n",
        "        tok_text = \" \".join(tok_tokens)\n",
        "\n",
        "        # de-tokenize WordPieces that have been split off\n",
        "        tok_text = tok_text.replace(\" ##\", \"\")\n",
        "        tok_text = tok_text.replace(\"##\", \"\")\n",
        "\n",
        "        # clean whitespace\n",
        "        tok_text = tok_text.strip()\n",
        "        tok_text = \" \".join(tok_text.split())\n",
        "        orig_text = \" \".join(orig_tokens)\n",
        "\n",
        "        final_text = get_final_text(tok_text, orig_text, do_lower_case)\n",
        "        if final_text in seen_predictions:\n",
        "          continue\n",
        "\n",
        "        seen_predictions[final_text] = True\n",
        "      else:\n",
        "        final_text = \"\"\n",
        "        seen_predictions[final_text] = True\n",
        "\n",
        "      nbest.append(\n",
        "          _NbestPrediction(\n",
        "              text=final_text,\n",
        "              start_logit=pred.start_logit,\n",
        "              end_logit=pred.end_logit))\n",
        "\n",
        "    # if we didn't inlude the empty option in the n-best, inlcude it\n",
        "    if \"\" not in seen_predictions:\n",
        "      nbest.append(\n",
        "          _NbestPrediction(\n",
        "              text=\"\", start_logit=null_start_logit,\n",
        "              end_logit=null_end_logit))\n",
        "    # in very rare edge cases we could have no valid predictions. So we\n",
        "    # just create a nonce prediction in this case to avoid failure\n",
        "    if not nbest:\n",
        "      nbest.append(\n",
        "          _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
        "\n",
        "    assert len(nbest) >= 1\n",
        "\n",
        "    total_scores = []\n",
        "    best_non_null_entry = None\n",
        "    for entry in nbest:\n",
        "      total_scores.append(entry.start_logit + entry.end_logit)\n",
        "      if not best_non_null_entry:\n",
        "        if entry.text:\n",
        "          best_non_null_entry = entry\n",
        "\n",
        "    probs = _compute_softmax(total_scores)\n",
        "\n",
        "    nbest_json = []\n",
        "    for (i, entry) in enumerate(nbest):\n",
        "      output = collections.OrderedDict()\n",
        "      output[\"text\"] = entry.text\n",
        "      output[\"probability\"] = probs[i]\n",
        "      output[\"start_logit\"] = entry.start_logit\n",
        "      output[\"end_logit\"] = entry.end_logit\n",
        "      nbest_json.append(output)\n",
        "\n",
        "    assert len(nbest_json) >= 1\n",
        "\n",
        "    # predict \"\" iff the null score - the score of best non-null > threshold\n",
        "    score_diff = score_null - best_non_null_entry.start_logit - (\n",
        "        best_non_null_entry.end_logit)\n",
        "    scores_diff_json[example.qas_id] = score_diff\n",
        "    if score_diff > null_score_diff_threshold:\n",
        "      all_predictions[example.qas_id] = \"\"\n",
        "    else:\n",
        "      all_predictions[example.qas_id] = best_non_null_entry.text\n",
        "\n",
        "    all_nbest_json[example.qas_id] = nbest_json\n",
        "\n",
        "  with tf.gfile.GFile(output_prediction_file, \"w\") as writer:\n",
        "    writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
        "\n",
        "  with tf.gfile.GFile(output_nbest_file, \"w\") as writer:\n",
        "    writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n",
        "\n",
        "  with tf.gfile.GFile(output_null_log_odds_file, \"w\") as writer:\n",
        "    writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n",
        "    \n",
        "def _get_best_indexes(logits, n_best_size):\n",
        "  \"\"\"\n",
        "  Get the n-best logits from a list\n",
        "  \"\"\"\n",
        "  index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  best_indexes = []\n",
        "  for i in range(len(index_and_score)):\n",
        "    if i >= n_best_size:\n",
        "      break\n",
        "    best_indexes.append(index_and_score[i][0])\n",
        "  return best_indexes\n",
        "\n",
        "def _compute_softmax(scores):\n",
        "  \"\"\"\n",
        "  Compute softmax probability over raw logits\n",
        "  \"\"\"\n",
        "  \n",
        "  if not scores:\n",
        "    return []\n",
        "\n",
        "  max_score = None\n",
        "  for score in scores:\n",
        "    if max_score is None or score > max_score:\n",
        "      max_score = score\n",
        "\n",
        "  exp_scores = []\n",
        "  total_sum = 0.0\n",
        "  for score in scores:\n",
        "    x = math.exp(score - max_score)\n",
        "    exp_scores.append(x)\n",
        "    total_sum += x\n",
        "\n",
        "  probs = []\n",
        "  for score in exp_scores:\n",
        "    probs.append(score / total_sum)\n",
        "  return probs\n",
        "\n",
        "def get_final_text(pred_text, orig_text, do_lower_case):\n",
        "  \"\"\"\n",
        "  Project the tokenized prediction back to the original text\n",
        "  \"\"\"\n",
        "\n",
        "  def _strip_spaces(text):\n",
        "    ns_chars = []\n",
        "    ns_to_s_map = collections.OrderedDict()\n",
        "    for (i, c) in enumerate(text):\n",
        "      if c == \" \":\n",
        "        continue\n",
        "      ns_to_s_map[len(ns_chars)] = i\n",
        "      ns_chars.append(c)\n",
        "    ns_text = \"\".join(ns_chars)\n",
        "    return (ns_text, ns_to_s_map)\n",
        "\n",
        "  tokenizer = tokenization.BasicTokenizer(do_lower_case=do_lower_case)\n",
        "\n",
        "  tok_text = \" \".join(tokenizer.tokenize(orig_text))\n",
        "\n",
        "  start_position = tok_text.find(pred_text)\n",
        "  if start_position == -1:\n",
        "    tf.logging.info(\n",
        "        \"Unable to find text: '%s' in '%s'\" % (pred_text, orig_text))\n",
        "    return orig_text\n",
        "  end_position = start_position + len(pred_text) - 1\n",
        "\n",
        "  (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n",
        "  (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n",
        "\n",
        "  if len(orig_ns_text) != len(tok_ns_text):\n",
        "    tf.logging.info(\"Length not equal after stripping spaces: '%s' vs '%s'\",\n",
        "                    orig_ns_text, tok_ns_text)\n",
        "    return orig_text\n",
        "\n",
        "  tok_s_to_ns_map = {}\n",
        "  for (i, tok_index) in six.iteritems(tok_ns_to_s_map):\n",
        "    tok_s_to_ns_map[tok_index] = i\n",
        "\n",
        "  orig_start_position = None\n",
        "  if start_position in tok_s_to_ns_map:\n",
        "    ns_start_position = tok_s_to_ns_map[start_position]\n",
        "    if ns_start_position in orig_ns_to_s_map:\n",
        "      orig_start_position = orig_ns_to_s_map[ns_start_position]\n",
        "\n",
        "  if orig_start_position is None:\n",
        "    tf.logging.info(\"Couldn't map start position\")\n",
        "    return orig_text\n",
        "\n",
        "  orig_end_position = None\n",
        "  if end_position in tok_s_to_ns_map:\n",
        "    ns_end_position = tok_s_to_ns_map[end_position]\n",
        "    if ns_end_position in orig_ns_to_s_map:\n",
        "      orig_end_position = orig_ns_to_s_map[ns_end_position]\n",
        "\n",
        "  if orig_end_position is None:\n",
        "    tf.logging.info(\"Couldn't map end position\")\n",
        "    return orig_text\n",
        "\n",
        "  output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n",
        "  return output_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQrIR5E8r3cu",
        "colab_type": "text"
      },
      "source": [
        "## Make predictions on SQuAD v2.0 dev set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcHr02-9TrHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_file = os.path.join(root, 'dev-v2.0.json')\n",
        "checkpoint_path = os.path.join(root, 'output/model.ckpt-xxxxx')\n",
        "eval_examples = read_examples(input_file=predict_file, is_training=False)\n",
        "\n",
        "eval_writer = run_squad.FeatureWriter(\n",
        "    filename=os.path.join(output_dir, \"eval.tf_record\"),\n",
        "    is_training=False)\n",
        "eval_features = []\n",
        "\n",
        "def append_feature(feature):\n",
        "    eval_features.append(feature)\n",
        "    eval_writer.process_feature(feature)\n",
        "\n",
        "run_squad.convert_examples_to_features(\n",
        "    examples=eval_examples,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=max_seq_length,\n",
        "    doc_stride=doc_stride,\n",
        "    max_query_length=max_query_length,\n",
        "    is_training=False,\n",
        "    output_fn=append_feature)\n",
        "eval_writer.close()\n",
        "\n",
        "tf.logging.info(\"***** Running predictions *****\")\n",
        "tf.logging.info(\"  Num orig examples = %d\", len(eval_examples))\n",
        "tf.logging.info(\"  Num split examples = %d\", len(eval_features))\n",
        "tf.logging.info(\"  Batch size = %d\", predict_batch_size)\n",
        "\n",
        "predict_input_fn = run_squad.input_fn_builder(\n",
        "    input_file=eval_writer.filename,\n",
        "    seq_length=max_seq_length,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)\n",
        "\n",
        "RawResult = collections.namedtuple(\"RawResult\", [\"unique_id\", \"start_logits\", \"end_logits\"])\n",
        "all_results = []\n",
        "for result in estimator.predict(predict_input_fn, yield_single_examples=True, checkpoint_path=checkpoint_path):\n",
        "  if len(all_results) % 1000 == 0:\n",
        "    tf.logging.info(\"Processing example: %d\" % (len(all_results)))\n",
        "  unique_id = int(result[\"unique_ids\"])\n",
        "  start_logits = [float(x) for x in result[\"start_logits\"].flat]\n",
        "  end_logits = [float(x) for x in result[\"end_logits\"].flat]\n",
        "  all_results.append(RawResult(unique_id=unique_id, start_logits=start_logits, end_logits=end_logits))\n",
        "\n",
        "output_prediction_file = os.path.join(root, \"predictions.json\")\n",
        "output_nbest_file = os.path.join(root, \"nbest_predictions.json\")\n",
        "output_null_log_odds_file = os.path.join(root, \"null_odds.json\")\n",
        "\n",
        "run_squad.write_predictions(eval_examples, eval_features, all_results,\n",
        "                  n_best_size, max_answer_length,\n",
        "                  do_lower_case, output_prediction_file,\n",
        "                  output_nbest_file, output_null_log_odds_file)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}